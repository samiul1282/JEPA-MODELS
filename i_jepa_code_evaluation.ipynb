{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65f3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 0.  Setup (run once per machine)\n",
    "# ==============================================================\n",
    "# !pip install -q torch torchvision albumentations timm torchinfo pyyaml tqdm pillow\n",
    "# !wget -q https://github.com/BRISC-Dataset/BRISC2025/archive/refs/heads/main.zip\n",
    "# !unzip -q main.zip && mv BRISC2025-main brisc2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddd17da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1.  Imports\n",
    "# ==============================================================\n",
    "import os, yaml, cv2, math, random, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937bbc72",
   "metadata": {},
   "source": [
    "# 2.  Hyper-parameters & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978fafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'img_size': 448,\n",
    "    'patch_size': 16,\n",
    "    'num_classes': 2,                 # background + tumor\n",
    "    'batch_size': 16,                 # reduce if OOM\n",
    "    'lr': 1e-3,\n",
    "    'epochs': 30,\n",
    "    'weight_decay': 1e-4,\n",
    "    'num_workers': 4,\n",
    "    'seed': 42,\n",
    "    'root': Path('./brisc2025/segmentation_task'),\n",
    "    'weights': 'IN1K-vit.h.16-448px-300e.pth.tar',  # I-JEPA ckpt\n",
    "}\n",
    "# Download frozen I-JEPA weights once:\n",
    "# !wget -q https://github.com/facebookresearch/ijepa/releases/download/v1.0/IN1K-vit.h.16-448px-300e.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65588a",
   "metadata": {},
   "source": [
    "# 3.  Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3050b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "\n",
    "# ==============================================================\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed(cfg['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94d2e3",
   "metadata": {},
   "source": [
    "# 4.  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0303448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transforms=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.mask_dir = Path(mask_dir)\n",
    "        self.ids = sorted([p.stem for p in self.img_dir.glob('*.jpg')])\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.ids[idx]\n",
    "        img = cv2.imread(str(self.img_dir/f'{name}.jpg'))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(self.mask_dir/f'{name}.png'), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 200).astype(np.uint8)          # binarise\n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=img, mask=mask)\n",
    "            img, mask = aug['image'], aug['mask']\n",
    "        return img, mask.long()\n",
    "\n",
    "train_aug = A.Compose([\n",
    "    A.Resize(cfg['img_size'], cfg['img_size']),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ColorJitter(0.2, 0.2, 0.2, 0.1, p=0.5),\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "val_aug = A.Compose([\n",
    "    A.Resize(cfg['img_size'], cfg['img_size']),\n",
    "    A.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_ds = SegDataset(cfg['root']/'train'/'images', cfg['root']/'train'/'masks', train_aug)\n",
    "val_ds   = SegDataset(cfg['root']/'test'/'images',  cfg['root']/'test'/'masks',  val_aug)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True,\n",
    "                          num_workers=cfg['num_workers'], pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg['batch_size'], shuffle=False,\n",
    "                          num_workers=cfg['num_workers'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65c345",
   "metadata": {},
   "source": [
    "# 5.  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac67b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 5.  Model  (fixed for your timm list)\n",
    "# ==============================================================\n",
    "# 5-a  Load the I-JEPA backbone that *is* in your timm registry\n",
    "backbone = timm.create_model(\n",
    "    'vit_huge_patch16_gap_448.in1k_ijepa',  # <-- correct tag in your list\n",
    "    pretrained=True,                        # loads the I-JEPA weights\n",
    "    num_classes=0                           # drop cls head\n",
    ")\n",
    "\n",
    "# 5-b  (optional) overwrite with your local checkpoint if you want\n",
    "# ckpt = torch.load(cfg['weights'], map_location='cpu')['encoder']\n",
    "# ckpt = {k.replace('module.', ''): v for k, v in ckpt.items()}\n",
    "# backbone.load_state_dict(ckpt, strict=True)\n",
    "\n",
    "# 5-c  Freeze backbone\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "backbone.eval().to(device)\n",
    "\n",
    "# 5-d  Light-weight decode head (unchanged)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_ch, num_cls):\n",
    "        super().__init__()\n",
    "        self.head = nn.Conv2d(in_ch, num_cls, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.head(x)\n",
    "\n",
    "class JepaSeg(nn.Module):\n",
    "    def __init__(self, backbone, decoder):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.decoder  = decoder\n",
    "    def forward(self, x):\n",
    "        # I-JEPA models return patch tokens directly\n",
    "        patches = self.backbone.forward_features(x)   # (B, N, D)\n",
    "        B, N, D = patches.shape\n",
    "        h = w = int(math.sqrt(N))                     # 28 for 448 px / 16\n",
    "        patches = patches.view(B, h, w, D).permute(0, 3, 1, 2)\n",
    "        logits  = self.decoder(patches)               # (B, num_cls, 28, 28)\n",
    "        return logits\n",
    "\n",
    "model = JepaSeg(backbone, Decoder(1280, cfg['num_classes'])).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20ba78",
   "metadata": {},
   "source": [
    "# 6.  Loss & optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa9e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.decoder.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860cc58",
   "metadata": {},
   "source": [
    "# 7.  Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1da8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_pytorch(pred, target, num_classes=2, eps=1e-6):\n",
    "    # pred: (B,H,W) after argmax\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        intersect = ((pred == cls) & (target == cls)).sum((1,2)).float()\n",
    "        union = ((pred == cls) | (target == cls)).sum((1,2)).float()\n",
    "        iou = (intersect + eps) / (union + eps)\n",
    "        ious.append(iou.mean().item())\n",
    "    return np.mean(ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059f3bc",
   "metadata": {},
   "source": [
    "# 8.  Training / validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d36725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/246 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "best_iou = 0.\n",
    "for epoch in range(1, cfg['epochs']+1):\n",
    "    # ---- train ----\n",
    "    model.train()\n",
    "    backbone.eval()              # keep frozen\n",
    "    train_loss, train_iou = [], []\n",
    "    for img, mask in tqdm(train_loader, leave=False):\n",
    "        img, mask = img.to(device), mask.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(img)                       # (B,2,28,28)\n",
    "        logits = nn.functional.interpolate(logits, size=mask.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        loss = criterion(logits, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        pred = logits.argmax(1)\n",
    "        train_iou.append(iou_pytorch(pred, mask))\n",
    "    # ---- val ----\n",
    "    model.eval()\n",
    "    val_loss, val_iou = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, mask in val_loader:\n",
    "            img, mask = img.to(device), mask.to(device)\n",
    "            logits = model(img)\n",
    "            logits = nn.functional.interpolate(logits, size=mask.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            val_loss.append(criterion(logits, mask).item())\n",
    "            val_iou.append(iou_pytorch(logits.argmax(1), mask))\n",
    "    # ---- log ----\n",
    "    print(f'E{epoch:02d} | '\n",
    "          f'train loss {np.mean(train_loss):.4f} mIoU {np.mean(train_iou):.4f} | '\n",
    "          f'val loss {np.mean(val_loss):.4f} mIoU {np.mean(val_iou):.4f}')\n",
    "    if np.mean(val_iou) > best_iou:\n",
    "        best_iou = np.mean(val_iou)\n",
    "        torch.save(model.state_dict(), 'best_jepa_seg.pth')\n",
    "        print('  â†‘ best model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381aba8",
   "metadata": {},
   "source": [
    "# 9.  Inference helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def infer(image_path, weight_path='best_jepa_seg.pth'):\n",
    "    model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "    model.eval()\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    aug = val_aug(image=img)['image'].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(aug)\n",
    "        mask = logits.argmax(1).squeeze(0).cpu().numpy()\n",
    "    return mask\n",
    "\n",
    "# Visualise\n",
    "mask = infer(r'brisc2025\\segmentation_task\\test\\images\\brisc2025_test_00001_gl_ax_t1.jpg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mask); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb507dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
